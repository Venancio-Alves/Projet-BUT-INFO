import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import pandas as pd
import re
import datetime
import hashlib
import os
import time
from bson.objectid import ObjectId

# Variables globales pour le suivi de l'√©tat
is_running = False
stop_requested = False
progress = None

# Fonction principale qui peut √™tre appel√©e depuis l'interface d'administration
def run_scraper(db_connection=None, source_file=None, max_urls=None):
    """
    Fonction principale de scraping qui peut √™tre appel√©e depuis l'interface d'administration
    
    Args:
        db_connection: Une connexion MongoDB existante (facultatif)
        source_file: Chemin vers le fichier Excel contenant les URLs (facultatif)
        max_urls: Nombre maximum d'URLs √† traiter (facultatif)
    
    Returns:
        dict: Statistiques sur les r√©sultats du scraping
    """
    global is_running, stop_requested, progress
    
    # Pr√©venir l'appel r√©cursif
    if is_running:
        print("WARNING: Une instance de scraping est d√©j√† en cours d'ex√©cution!")
        return {
            'total_processed': 0,
            'new_added': 0,
            'updated': 0,
            'unchanged': 0,
            'errors': 1,
            'duration': 0,
            'error': 'Scraping already running'
        }
    
    try:
        # R√©initialiser les variables de contr√¥le
        is_running = True
        stop_requested = False
        start_time = time.time()
        
        print("Initialisation du processus de scraping complet...")
        
        # Connexion √† MongoDB - FIX: Ne pas utiliser de test bool√©en sur db_connection
        # Pour pymongo, db_connection ne doit pas √™tre test√© directement avec if db_connection:
        db = None
        if db_connection is not None:
            print("Utilisation de la connexion MongoDB fournie")
            db = db_connection
        else:
            print("Cr√©ation d'une nouvelle connexion MongoDB")
            client = MongoClient("mongodb://localhost:27017/")
            db_name = "medicsearch"
            db = client[db_name]
        
        collection = db['medicines']
        metadata_collection = db['metadata']
        
        # V√©rifier s'il existe une t√¢che de scraping en cours
        scraping_task = metadata_collection.find_one({"_id": "scraping_current_task"})
        if scraping_task and scraping_task.get('in_progress', False):
            # Reprendre une t√¢che existante
            print("Reprise d'une t√¢che de scraping pr√©c√©dente...")
            urls = scraping_task.get('remaining_urls', [])
            processed_urls = scraping_task.get('processed_urls', [])
            stats = scraping_task.get('current_stats', {
                'total_processed': len(processed_urls),
                'new_added': 0,
                'updated': 0,
                'unchanged': 0,
                'errors': 0,
                'duration': 0
            })
            
            # Si aucune URL restante, recommencer depuis le d√©but
            if not urls:
                print("Aucune URL restante dans la t√¢che pr√©c√©dente. Recommencement depuis le d√©but.")
                # R√©initialiser la t√¢che et commencer une nouvelle
                scraping_task = None
        else:
            scraping_task = None
            processed_urls = []
            stats = {
                'total_processed': 0,
                'new_added': 0,
                'updated': 0,
                'unchanged': 0,
                'errors': 0,
                'duration': 0
            }
    
        # Si aucune t√¢che en cours ou si on red√©marre, charger les URLs depuis le fichier
        # Toujours commencer une nouvelle t√¢che (scraping complet)
        scraping_task = None
        processed_urls = []
        stats = {
            'total_processed': 0,
            'new_added': 0,
            'updated': 0,
            'unchanged': 0,
            'errors': 0,
            'duration': 0
        }

        # Charger les URLs depuis le fichier Excel
        if not source_file:
            # Chercher dans le r√©pertoire du script
            script_dir = os.path.dirname(os.path.abspath(__file__))
            possible_files = [
                os.path.join(script_dir, "liens_R.xlsx"),
                os.path.join(script_dir, "..", "liens_R.xlsx"),
                os.path.join(script_dir, "..", "data", "liens_R.xlsx"),
            ]
            
            for file_path in possible_files:
                if os.path.exists(file_path):
                    source_file = file_path
                    print(f"Fichier source trouv√©: {file_path}")
                    break
        
        if not source_file or not os.path.exists(source_file):
            # Si nous avons des m√©dicaments existants, mettons-les √† jour au lieu d'√©chouer
            existing_count = collection.count_documents({})
            if existing_count > 0:
                print(f"Aucun fichier source trouv√©, mais {existing_count} m√©dicaments existent d√©j√†. Mise √† jour des m√©dicaments existants...")
                urls = []
                for med in collection.find({}, {"url": 1}).limit(max_urls if max_urls else 1000):
                    if "url" in med:
                        urls.append(med["url"])
                if not urls:
                    raise FileNotFoundError("Aucune URL trouv√©e dans la base de donn√©es existante")
            else:
                raise FileNotFoundError("Fichier Excel contenant les URLs non trouv√© et aucun m√©dicament existant")
        else:
            print(f"Chargement des URLs depuis le fichier: {source_file}")
            df = pd.read_excel(source_file)
            if 'liens' not in df.columns:
                raise ValueError("Le fichier Excel ne contient pas de colonne 'liens'.")
                
            urls = df["liens"].tolist()
            print(f"{len(urls)} URLs charg√©es depuis le fichier Excel")
        
        if max_urls and max_urls > 0:
            print(f"Limitation √† {max_urls} URLs maximum")
            urls = urls[:max_urls]
    
        # Initialiser ou mettre √† jour la t√¢che de scraping
        total_urls = len(urls) + len(processed_urls)
        print(f"Total URLs √† traiter: {total_urls}")
        
        if total_urls == 0:
            print("Aucune URL √† traiter. Arr√™t du scraping.")
            is_running = False
            return {
                'total_processed': 0,
                'new_added': 0,
                'updated': 0,
                'unchanged': 0,
                'errors': 0,
                'duration': 0,
                'message': 'No URLs to process'
            }
        
        metadata_collection.update_one(
            {"_id": "scraping_current_task"},
            {
                "$set": {
                    "in_progress": True,
                    "start_time": datetime.datetime.now(),
                    "total_urls": total_urls,
                    "remaining_urls": urls,
                    "processed_urls": processed_urls,
                    "current_stats": stats
                }
            },
            upsert=True
        )
        
        # Mettre √† jour la progression globale pour le suivi de l'interface
        progress = {
            "percent": int(len(processed_urls) / total_urls * 100) if total_urls > 0 else 0,
            "current": len(processed_urls),
            "total": total_urls,
            "new_added": stats.get('new_added', 0),
            "updated": stats.get('updated', 0),
            "unchanged": stats.get('unchanged', 0),
            "errors": stats.get('errors', 0)
        }
        
        # Traitement des URLs
        for url in urls[:]:
            # V√©rifier si l'arr√™t a √©t√© demand√©
            if stop_requested:
                print("Arr√™t demand√© par l'utilisateur. Sauvegarde de l'√©tat...")
                # Sauvegarder l'√©tat actuel
                metadata_collection.update_one(
                    {"_id": "scraping_current_task"},
                    {
                        "$set": {
                            "last_updated": datetime.datetime.now(),
                            "remaining_urls": urls,
                            "processed_urls": processed_urls,
                            "current_stats": stats,
                            "stopped_by_user": True  # Marquer comme arr√™t√© manuellement
                        }
                    }
                )
                is_running = False
                return stats
                
            stats['total_processed'] += 1
            processed_urls.append(url)
            urls.remove(url)
            
            # Mettre √† jour le pourcentage
            if total_urls > 0:  # Pr√©venir la division par z√©ro
                progress["percent"] = int(len(processed_urls) / total_urls * 100)
            progress["current"] = len(processed_urls)
            progress["new_added"] = stats.get('new_added', 0)
            progress["updated"] = stats.get('updated', 0)
            progress["unchanged"] = stats.get('unchanged', 0)
            progress["errors"] = stats.get('errors', 0)
            
            print(f"Progression: {progress['percent']}% - URL {progress['current']}/{total_urls}")
            
            try:
                response = requests.get(url)
                response.raise_for_status()
                
                # Gestion des encodages
                soup = None
                try:
                    soup = BeautifulSoup(response.content, 'html.parser')
                except:
                    for encoding in ['utf-8', 'latin-1', 'windows-1252', 'iso-8859-1']:
                        try:
                            soup = BeautifulSoup(response.content.decode(encoding), 'html.parser')
                            break
                        except:
                            continue
                
                if not soup:
                    raise ValueError("Impossible de d√©coder le contenu de la page")
                    
                # Extractions
                main_title = extract_medicine_title(soup)
                substance_dosage_data = extract_substances_and_dosages(soup)
                update_date = extract_update_date(soup)
                
                # Cr√©er le document
                document = {
                    "url": url,
                    "title": main_title,
                    "medicine_details": {
                        "substances_actives": substance_dosage_data["substances_actives"],
                        "laboratoire": extract_laboratory(soup),
                        "dosages": substance_dosage_data["dosages"],
                        "forme": extract_pharmaceutical_form(soup)
                    },
                    "update_date": update_date,
                    "last_scraped": datetime.datetime.now()  # Ajout de la date de scraping
                }
                
                # Extraire les sections
                try:
                    document["sections"] = extract_sections(soup)
                except Exception as e:
                    print(f"Erreur lors de l'extraction des sections pour {url}: {str(e)}")
                    document["sections"] = []
                
                # G√©n√©rer un hash du contenu pour v√©rifier si le m√©dicament a chang√©
                content_hash = generate_content_hash(document)
                document["content_hash"] = content_hash
                
                # V√©rifier si le m√©dicament existe d√©j√†
                existing = collection.find_one({"url": url})
                
                if existing:
                    if "content_hash" in existing and existing["content_hash"] == content_hash:
                        # Le contenu n'a pas chang√©, mettre √† jour uniquement la date de scraping
                        collection.update_one(
                            {"_id": existing["_id"]},
                            {"$set": {"last_scraped": datetime.datetime.now()}}
                        )
                        stats['unchanged'] += 1
                        progress['unchanged'] += 1
                        print(f"M√©dicament inchang√©: {main_title}")
                    else:
                        # Le contenu a chang√©, mettre √† jour
                        collection.update_one(
                            {"_id": existing["_id"]},
                            {"$set": document}
                        )
                        stats['updated'] += 1
                        progress['updated'] += 1
                        print(f"M√©dicament mis √† jour: {main_title}")
                else:
                    # Nouveau m√©dicament
                    collection.insert_one(document)
                    stats['new_added'] += 1
                    progress['new_added'] += 1
                    print(f"Nouveau m√©dicament ajout√©: {main_title}")
            
            except Exception as e:
                print(f"Erreur lors du traitement de {url}: {str(e)}")
                stats['errors'] = stats.get('errors', 0) + 1
                if 'errors' in progress:
                    progress['errors'] = stats['errors']
            
            # Mettre √† jour r√©guli√®rement l'√©tat de la t√¢che dans la base de donn√©es
            if len(processed_urls) % 10 == 0 or len(urls) == 0:
                print(f"Sauvegarde de l'√©tat du scraping... ({len(processed_urls)}/{total_urls})")
                metadata_collection.update_one(
                    {"_id": "scraping_current_task"},
                    {
                        "$set": {
                            "last_updated": datetime.datetime.now(),
                            "remaining_urls": urls,
                            "processed_urls": processed_urls,
                            "current_stats": stats
                        }
                    }
                )
            
            print("-" * 50)
        
        # Calculer la dur√©e totale
        stats['duration'] = round(time.time() - start_time)
        
        # Mettre √† jour les m√©tadonn√©es et marquer la t√¢che comme termin√©e
        current_time = datetime.datetime.now()
        print(f"Mise √† jour des m√©tadonn√©es de scraping...")
        
        metadata_collection.update_one(
            {"_id": "scraping_current_task"},
            {
                "$set": {
                    "in_progress": False,
                    "completed_at": current_time,
                    "duration": stats['duration']
                }
            }
        )
        
        # Mettre √† jour les m√©tadonn√©es g√©n√©rales
        metadata_collection.update_one(
            {"_id": "scraping_metadata"},
            {
                "$set": {
                    "last_update": current_time,
                    "total_medicines": collection.count_documents({}),
                    "last_scraping_results": stats
                }
            },
            upsert=True
        )
        
        print(f"Scraping termin√©!")
        print(f"Total trait√©: {stats['total_processed']} | Nouveaux: {stats.get('new_added', 0)} | Mis √† jour: {stats.get('updated', 0)} | Inchang√©s: {stats.get('unchanged', 0)} | Erreurs: {stats.get('errors', 0)}")
        print(f"Dur√©e totale: {stats['duration']} secondes")
        
        return stats
        
    except Exception as e:
        print(f"ERREUR CRITIQUE dans run_scraper: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'total_processed': 0,
            'new_added': 0,
            'updated': 0,
            'unchanged': 0,
            'errors': 1,
            'duration': 0,
            'error_message': str(e)
        }
    finally:
        # Toujours r√©initialiser les variables globales, m√™me en cas d'erreur
        is_running = False
        progress = None

def generate_content_hash(document):
    """G√©n√®re un hash du contenu essentiel pour d√©tecter les changements"""
    # S√©lectionner les champs importants pour la comparaison
    content_string = (
        document.get('title', '') + 
        str(document.get('update_date', '')) + 
        str(document.get('medicine_details', {})) + 
        str([
            {k: v for k, v in section.items() if k != 'subsections'} 
            for section in document.get('sections', [])
        ])
    )
    return hashlib.md5(content_string.encode('utf-8')).hexdigest()

def extract_update_date(soup):
    """Extrait la date de mise √† jour du document"""
    update_date = "Date not found"
    ansm_date_pattern = soup.find(string=lambda text: text and "ANSM - Mis √† jour le :" in text)
    
    if ansm_date_pattern:
        update_date = ansm_date_pattern.split("ANSM - Mis √† jour le :")[1].strip()
    else:
        update_date_element = soup.find('div', id='menuhaut')
        if update_date_element:
            update_date = update_date_element.get_text(strip=True)
            if "mise √† jour" in update_date:
                update_date = update_date.split("mise √† jour")[1].strip()
            elif "mise" in update_date:
                update_date = update_date.split("mise")[1].strip()
    
    return update_date.replace("le ", "").strip()

def extract_text_content(element):
    """Extrait le texte et les attributs de formatage importants"""
    text = re.sub(r'\s+', ' ', element.get_text(strip=False)).strip()
    
    formatting = {
        "bold": False, "italic": False, "underline": False,
        "list_type": None, "alignment": "left"
    }
    
    # D√©tection de formatage
    if element.name in ['strong', 'b'] or element.find(['strong', 'b']):
        formatting["bold"] = True
    
    if element.name in ['em', 'i'] or element.find(['em', 'i']):
        formatting["italic"] = True
    
    if 'class' in element.attrs:
        classes = ' '.join(element['class'])
        if 'gras' in classes or 'AmmCorpsTexteGras' in classes:
            formatting["bold"] = True
        if 'italique' in classes:
            formatting["italic"] = True
        if 'souligne' in classes:
            formatting["underline"] = True
        if 'AmmListePuces' in classes:
            formatting["list_type"] = "bullet"
        if 'center' in classes or 'text-align:center' in str(element):
            formatting["alignment"] = "center"
    
    if element.name == 'li' or (element.parent and element.parent.name in ['ul', 'ol']):
        formatting["list_type"] = "bullet" if element.parent and element.parent.name == 'ul' else "numbered"
    
    return {"text": text, "formatting": formatting}

def extract_sections(soup):
    """
    Construit une hi√©rarchie des sections √† partir de <a name="RcpDenomination">.
    Si non trouv√©, commence apr√®s <p class="DateNotif">.
    L'extraction s'arr√™te √† <a name="RcpInstPrepRadioph">.
    """
    root_sections = []    # Liste des sections principales
    stack = []            # Pile pour g√©rer la hi√©rarchie
    processed_elements = set()  # Pour suivre les √©l√©ments d√©j√† trait√©s
    
    body = soup.find('body')
    if not body:
        return root_sections

    # Trouver le point de d√©part
    start_found = False
    elements = body.find_all(['a', 'p', 'div', 'table'], recursive=True)
    
    for i, element in enumerate(elements):
        # Point de d√©part principal : RcpDenomination
        if element.name == 'a' and element.get('name') == 'RcpDenomination':
            start_found = True
            continue
        # Point de d√©part alternatif : apr√®s DateNotif
        elif not start_found and element.name == 'p' and element.has_attr('class') and 'DateNotif' in element['class']:
            start_found = True
            continue
        
        # Ne rien traiter avant le point de d√©part
        if not start_found:
            continue
            
        # Arr√™ter √† l'ancre de fin
        if element.name == 'a' and element.get('name') == 'RcpInstPrepRadioph':
            break

        # Ne pas traiter les √©l√©ments d√©j√† trait√©s
        if element in processed_elements:
            continue

        # D√©tecter un titre de section
        if element.name == 'p' and element.has_attr('class'):
            section_class = next((cls for cls in element['class'] if cls.startswith("AmmAnnexeTitre")), None)
            if section_class:
                match = re.search(r"AmmAnnexeTitre(\d+)(Bis)?", section_class)
                if match:
                    level = int(match.group(1))
                    a_tag = element.find('a')
                    title = a_tag.get_text(strip=True) if a_tag else element.get_text(strip=True)
                    
                    # Cr√©er la nouvelle section sans le champ "level"
                    new_section = {"title": title, "content": [], "subsections": []}
                    
                    # G√©rer la hi√©rarchie
                    while stack and stack[-1].get("level", 0) >= level:
                        stack.pop()
                    if stack:
                        stack[-1]["subsections"].append(new_section)
                    else:
                        root_sections.append(new_section)
                    # Garder le level temporairement dans la stack pour la comparaison
                    new_section["level"] = level
                    stack.append(new_section)
                    continue

        # Traiter le contenu
        if element.name == 'table':
            # Extraction du contenu de tableau int√©gr√©e directement
            try:
                rows = element.find_all("tr")
                
                # Marquer tous les √©l√©ments du tableau comme trait√©s
                for row in rows:
                    for cell in row.find_all(['td', 'th']):
                        processed_elements.add(cell)
                        # Marquer √©galement tous les √©l√©ments enfants
                        for child in cell.find_all(True):
                            processed_elements.add(child)
                
                headers = []
                header_row = element.find("thead")
                if header_row and header_row.find_all("th"):
                    headers = [th.get_text(strip=True) for th in header_row.find_all("th")]
                
                table_data = []
                for row in rows:
                    cols = row.find_all(["td", "th"])
                    row_data = [col.get_text(strip=True) for col in cols]
                    if any(cell.strip() for cell in row_data):
                        table_data.append(row_data)
                
                content_data = {"table": table_data}
                
                if headers:
                    content_data["headers"] = headers
                
                caption = element.find("caption")
                if caption:
                    content_data["caption"] = caption.get_text(strip=True)
                    processed_elements.add(caption)
            except Exception as e:
                content_data = {"error": f"Erreur d'extraction de tableau: {str(e)}"}
        elif element.name in ['p', 'div']:
            if element.has_attr('class') and any(cls.startswith("AmmAnnexeTitre") for cls in element['class']):
                continue
            content_data = extract_text_content(element)
        else:
            continue
        
        # Ajouter le contenu √† la derni√®re section
        if stack:
            stack[-1]["content"].append(content_data)
        elif root_sections:
            root_sections[-1]["content"].append(content_data)
        else:
            root_sections.append({
                "title": "Contenu non sectionn√©",
                "content": [content_data],
                "subsections": []
            })
    
    # Nettoyer les sections en supprimant le champ level
    def clean_sections(sections):
        for section in sections:
            if "level" in section:
                del section["level"]
            clean_sections(section["subsections"])
    
    clean_sections(root_sections)
    return root_sections

def extract_medicine_title(soup):
    """Extrait directement le titre du m√©dicament"""
    denomination_section = soup.find('a', {'name': 'RcpDenomination'})
    if denomination_section:
        title_element = denomination_section.find_next('p', class_=lambda c: c and ('AmmCorpsTexteGras' in c or 'AmmDenomination' in c))
        if title_element:
            return re.sub(r'\s+', ' ', title_element.get_text(strip=True)).strip()
    
    # M√©thodes alternatives
    title_h1 = soup.find('h1', class_='textedeno')
    if title_h1:
        title_text = title_h1.get_text(strip=True)
        if " - " in title_text:
            title_text = title_text.split(" - ")[0].strip()
        return title_text
    
    # Derni√®re tentative
    for class_name in ['AmmDenomination', 'AmmCorpsTexteGras']:
        title_elements = soup.find_all('p', class_=class_name, limit=3)
        for element in title_elements:
            text = element.get_text(strip=True)
            if text and len(text) > 5:
                return re.sub(r'\s+', ' ', text).strip()
    
    return "Document sans titre"

def extract_laboratory(soup):
    """Extrait directement le laboratoire"""
    titulaire_section = soup.find('a', {'name': 'RcpTitulaireAmm'})
    if titulaire_section:
        # R√©cup√©rer pr√©cis√©ment le paragraphe qui contient le nom du laboratoire (g√©n√©ralement le premier ou deuxi√®me paragraphe apr√®s l'ancre)
        paragraphs = []
        current_elem = titulaire_section.parent
        # Obtenir les 3 paragraphes apr√®s la section de titre
        for _ in range(5):  # Cherche dans les 5 √©l√©ments suivants maximum
            current_elem = current_elem.find_next(['p', 'div'])
            if not current_elem:
                break
            paragraphs.append(current_elem)
        
        # Strat√©gie 1: Chercher sp√©cifiquement un √©l√©ment avec span class="gras"
        for paragraph in paragraphs:
            spans = paragraph.find_all('span', class_='gras')
            for span in spans:
                text = span.get_text(strip=True)
                # V√©rifier que ce n'est pas une adresse (ne contient pas de code postal)
                if text and not re.match(r'^\d{5}', text) and not re.search(r'\d{5}\s', text):
                    return text
        
        # Strat√©gie 2: Chercher le premier paragraphe qui contient du texte en gras
        for paragraph in paragraphs:
            # V√©rifier si le paragraphe a la classe AmmCorpsTexteGras ou contient un span gras
            if (paragraph.has_attr('class') and 'AmmCorpsTexteGras' in paragraph['class']) or paragraph.find('span', class_='gras'):
                text = paragraph.get_text(strip=True)
                # V√©rifier que ce n'est pas un titre, une date ou une adresse
                if not text.startswith(('7.', '8.', 'TITULAIRE', 'DATE')) and not re.match(r'^\d{5}', text):
                    return text
        
        # Strat√©gie 3: Prendre le premier paragraphe non vide qui n'est pas un titre
        for paragraph in paragraphs:
            text = paragraph.get_text(strip=True)
            # Exclure les titres et les adresses
            if (text and 
                not text.startswith(('7.', '8.', 'TITULAIRE', 'DATE')) and 
                not re.match(r'^\d{5}', text) and 
                not re.search(r'\b\d{5}\b', text) and  # Pas de code postal
                not any(word.lower() in text.lower() for word in ['rue', 'avenue', 'boulevard', 'cedex'])):  # Pas d'adresse
                return text.replace('LABORATOIRES', 'LABORATOIRES ').strip()  # Fix for cases where LABORATOIRES is stuck to the name
    
    return ""

def extract_substances_and_dosages(soup):
    """
    Extrait la premi√®re substance active et son dosage √† partir du premier √©l√©ment avec la classe 'AmmComposition'.
    """
    dosages = []
    substances = []

    # R√©cup√®re directement le premier paragraphe avec la classe 'AmmComposition'
    paragraph = soup.find('p', class_='AmmComposition')
    
    if paragraph:
        text = paragraph.get_text(strip=True)
        # Reduced logging to avoid console clutter
        # print(f"üîé Paragraphe trouv√©: {text}")

        # Regex pour extraire substance et dosage
        match = re.search(
            r"^(.*?)\.{3,}\s*([\d\s,]+(?:[.,]\d+)?\s*(?:mg|g|ml|¬µg|UI|U\.I\.|microgrammes|unit√©s|%))\s*$",
            text, re.UNICODE | re.IGNORECASE
        )
        if match:
            substance = match.group(1).strip()
            dosage = match.group(2).strip()

            # Nettoyage du nom de la substance : supprime les contenus entre parenth√®ses
            substance = re.sub(r'\s*\([^)]*\)', '', substance).strip()

            # print(f"‚úÖ Extraction r√©ussie : Substance '{substance}' - Dosage '{dosage}'")
            substances.append(substance)
            dosages.append(dosage)
        # else:
            # print("‚ö†Ô∏è Aucun dosage d√©tect√© dans ce texte.")
    # else:
        # print("‚ö†Ô∏è Aucune balise <p class='AmmComposition'> trouv√©e.")

    return {
        "substances_actives": substances,
        "dosages": dosages
    }

def extract_pharmaceutical_form(soup):
    """Extrait la forme pharmaceutique"""
    form_section = soup.find('a', {'name': 'RcpFormePharm'})
    if form_section:
        # Recherche plus g√©n√©rique pour trouver le premier paragraphe apr√®s la section
        form_paragraph = form_section.find_next('p')
        if form_paragraph:
            return form_paragraph.get_text(strip=True).rstrip('.')
    
    # Alternative via le titre
    title = extract_medicine_title(soup)
    if title and ',' in title:
        return title.split(',', 1)[1].strip()
    
    return ""

if __name__ == '__main__':
    print("Lancement du scraper en mode standalone...")
    run_scraper()
